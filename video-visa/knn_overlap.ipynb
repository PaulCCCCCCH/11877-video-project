{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56472cb3-9b3e-42e8-b1e4-7922fe19d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44112it [00:00, 1276980.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44112it [02:05, 352.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/44112 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m clip_model, preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m clip_text_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, txt \u001b[38;5;129;01min\u001b[39;00m tqdm(texts):\n\u001b[1;32m     74\u001b[0m     clip_text_emb \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mencode_text(clip\u001b[38;5;241m.\u001b[39mtokenize(txt)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     75\u001b[0m     clip_text_embeddings\u001b[38;5;241m.\u001b[39mappend(clip_text_emb)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from data.image_folder import make_dataset\n",
    "from data.visamm_dataset import make_mm_dataset\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "IS_MM = False\n",
    "\n",
    "dataroot = '../pre-processing/subset_classes'\n",
    "# dataroot = '../pre-processing/mmdata'\n",
    "\n",
    "resultsroot= './saved_results/five-classes/bear/latent_code/bear_256x512/test_latest/images'\n",
    "# resultsroot= './results/debug/latent_code/debug/test_latest/images'\n",
    "# resultsroot= './checkpoints/visamm-gpt-five/latent_code/debug/test_latest/images'\n",
    "\n",
    "\n",
    "if IS_MM:\n",
    "    paths = sorted(make_mm_dataset(dataroot, 1), key=lambda d: d[\"im_path\"])\n",
    "    paths, texts = [p[\"im_path\"] for p in paths], [p[\"transc\"] for p in paths]\n",
    "else:\n",
    "    paths = sorted(make_dataset(dataroot))\n",
    "\n",
    "# embeddings_to_plot = 500\n",
    "\n",
    "name2idx = {\n",
    "    'bicycling': 1,    \n",
    "    'drawing': 2,\n",
    "    'driving': 3,\n",
    "    'running': 4,\n",
    "    'surfing': 5\n",
    "}\n",
    "\n",
    "class_indices = []\n",
    "class_names = []\n",
    "image_embeddings = []\n",
    "\n",
    "# Loading classes\n",
    "for idx, path in tqdm(enumerate(paths)):\n",
    "    classname = path.split('/')[-3]\n",
    "    class_indices.append(name2idx[classname])\n",
    "    class_names.append(classname)\n",
    "print(\"classes loaded\")\n",
    "\n",
    "# Loading embeddings\n",
    "result_fnames = os.listdir(resultsroot)\n",
    "result_fnames.sort(key=lambda p: int(p.split(\"_\")[0]))\n",
    "for idx, fname in tqdm(enumerate(result_fnames)):\n",
    "    fpath = os.path.join(resultsroot, fname)\n",
    "    embedding = np.load(fpath)\n",
    "    image_embeddings.append(embedding)\n",
    "print(\"embeddings loaded\")\n",
    "\n",
    "data = list(zip(class_names, image_embeddings, class_indices))\n",
    "data.sort(key=lambda p: p[2])\n",
    "class_names = [p[0] for p in data]\n",
    "image_embeddings = torch.tensor([p[1].flatten() for p in data])\n",
    "class_indices = [p[2] for p in data]\n",
    "\n",
    "# Loading CLIP labels as ground truth\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device='cuda')\n",
    "clip_text_embeddings = []\n",
    "for img, txt in tqdm(texts):\n",
    "    clip_text_emb = clip_model.encode_text(clip.tokenize(txt).to('cuda'))\n",
    "    clip_text_embeddings.append(clip_text_emb)\n",
    "print(\"clip embeddings loaded\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b9b15a8-ba4b-4b49-a73d-6a6fe83b7e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor each produced embedding a, clip embedding b\\n    for each of a's 5 neighbors, b's 5 neighbors\\n        overlap += overlap\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Find nearest neighbors for each training data\n",
    "knn = NearestNeighbors(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(image_embeddings[13:26])\n",
    "\n",
    "_, neighbors = knn.kneighbors(image_embeddings[13:26]) \n",
    "metric = nn.CosineSimilarity()\n",
    "\n",
    "# Count shared nearest neighbors\n",
    "\"\"\"\n",
    "for each produced embedding a, clip embedding b\n",
    "    for each of a's 5 neighbors, b's 5 neighbors\n",
    "        overlap += overlap\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8028a795-1347-4246-aa3e-0a38e04c56a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "# n1: tested; n2: ground truth (clip)\n",
    "for l1, l2 in zip(n1, n2):\n",
    "    total += sum(np.in1d(l1, l2))\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da66439f-adc7-4d08-be27-ed39641824e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 11  2  0  1] [12  2  1  0  3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(l1, l2)\n",
    "sum(np.in1d(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6654c34-b78f-44a7-8011-b12c238aced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58855722-adb7-4675-ad6b-ff502ee011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b25d86-49f1-4133-b425-f8034e631193",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device='cuda')\n",
    "clip_text_emb = clip_model.encode_text(clip.tokenize(\"Hello, good morning\").to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aef1e7b-57e2-40a8-9261-abfb45764870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   886,  1119,   637, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cat([clip.tokenize(\"Good Morning one\"), clip.tokenize(\"Good morning two\")])\n",
    "clip.tokenize(\"Good Morning one\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
